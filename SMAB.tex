\begin{frame}
\frametitle{Stochastic Multi-Armed Bandit Problem}
\begin{itemize}
\item<1-> In stochastic multi-armed bandit problem we are presented with a finite set of actions or arms. 
\item<2-> The rewards for each of the arms is drawn from identical and independent distributions. 
\item<3-> The learner does not know the mean of the distributions, denoted by $\mu_{i}$. 
\item<4-> The learner has to find the optimal arm the mean of whose distribution is denoted by $\mu^{*}$ such that $\mu^{*}> \mu_{i}, \forall i\in A$.
\item<5-> The distributions for each of the arms are fixed throughout the time horizon. 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Basic Notations}
\begin{itemize}
\item<1-> Goal: To minimize Regret
\item<2->  Average reward of best action is $\mu^{*}$ and any other action $i$ as $\mu_{i}$. There are $K$ total actions. $T_{i}(n)$ is number of times tried action $i$ is executed till $n$-timesteps.
\item<3->  Cumulative Regret: The loss we suffer because of not pulling the optimal arm till the total number of timesteps  $n$. 
\begin{align*}
R_{n}=\mu^{*}n - \sum_{i\in A} \mu_{i}T_{i}(n),
\end{align*}
\item<4->  The expected regret of an algorithm after $n$ rounds can be written as
\begin{align*}
\E[R_{n}]= \sum_{i=1}^K \E[T_{i}(n)] \Delta_i,
\end{align*}
\item<4-> $\Delta_{i}=\mu^{*}-\mu_{i}$ denotes the gap between the means of the optimal arm and of the $i$-th arm. 
\end{itemize}
\end{frame}